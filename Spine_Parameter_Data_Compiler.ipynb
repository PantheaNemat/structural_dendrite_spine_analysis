{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "electoral-smooth",
   "metadata": {},
   "source": [
    " ## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-offense",
   "metadata": {},
   "source": [
    "# Spine Parameter Data Compiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-collectible",
   "metadata": {},
   "source": [
    "## With this script data on reconstructed dendrites and spines can be compiled into two data sets: <br>\n",
    "### - number and distribution of spines <br> \n",
    "### - morphology of spines <br>\n",
    "\n",
    "For information on reconstruction of dendrites and spines, we would like to refer to our protocol in Appendix 1 of this publication. <br>\n",
    "\n",
    "The following input compiled into one folder is needed: <br>\n",
    "1. This script needs to be stored into the folder\n",
    "2. Overview file linking Animal Number to the Condition, ending with _Overview.csv\n",
    "3. Folder containing data for number and distribution of spines, called AllS\n",
    "4. Folder containing data for morphology of spines, called SelS <br>\n",
    "(if no differentiation is being made between AllS and SelS data, please adjust all \"SelS\" in this script to \"Alls\")\n",
    "\n",
    "Folders containing data are named in the example folder in the following manner: <br>\n",
    "ExperimentNumber_AnimalNumber_SliceNumber_Left/RightHemisphere_ImagedChannels_SO/SRHippocampalLayer_<br>\n",
    "Deconvolved_AllSpines/SelectedSpines_ReactivationStatus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-walnut",
   "metadata": {},
   "source": [
    "## 1. Import Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-forum",
   "metadata": {},
   "source": [
    "#### 1.1 Install Pandas package if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "champion-storm",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/panthea/miniconda3/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Users/panthea/miniconda3/lib/python3.11/site-packages (from pandas) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/panthea/miniconda3/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/panthea/miniconda3/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/panthea/miniconda3/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/panthea/miniconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-battery",
   "metadata": {},
   "source": [
    "#### 1.2 Import Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "least-numbers",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-solomon",
   "metadata": {},
   "source": [
    "#### 2.1 Create 'Processed_data' folder\n",
    "First the Current Working Directory of the Script is being stored as the variable *script_folder*. <br>\n",
    "Next Python creates the *Processed_Data* folder within the same directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "illegal-defendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_folder = os.getcwd()\n",
    "folder_script = 'Processed_data'\n",
    "base_directory = os.path.join(script_folder, folder_script)\n",
    "os.makedirs(base_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-fantasy",
   "metadata": {},
   "source": [
    "## 2. Prepare data structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-refrigerator",
   "metadata": {},
   "source": [
    "#### 2.2 Create required subfolders\n",
    "Within the 'Processed_data' folder four subfolders are being constructed <br>\n",
    "'Meta_data' - this subfolder will contain the meta data file in which subject number is linked to the experimental condition<br>\n",
    "'Dendrite' - this subfolder will contain data files on dendrite level - spine density and dendrite length <br> \n",
    "'Morph' - this subfolder will contain data files related to spine morphology <br>\n",
    "'Distr' - this subfolder will contain data files related to distribution of spines along the dendrite <br>\n",
    "'Output' - this subfolder will contain the combined data files resulting of this pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "structural-stranger",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_to_make = ['Meta_data', 'Dendrite', 'Morph', 'Distr', 'Output']\n",
    "for folder in folder_to_make:\n",
    "    sub_directory = os.path.join(base_directory, folder)\n",
    "    os.makedirs(sub_directory, exist_ok=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-knock",
   "metadata": {},
   "source": [
    "#### 2.3 Setting up paths to subfolders\n",
    "Creates specific directories to each subfolder in 'Procssed Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "invalid-ratio",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_morph = os.path.join(base_directory, \"Morph\") \n",
    "directory_distr = os.path.join(base_directory, \"Distr\") \n",
    "directory_dendrite = os.path.join(base_directory, \"Dendrite\") \n",
    "directory_distr_dendrite = os.path.join(base_directory, \"Distr_and_Dendrite\")\n",
    "directory_meta_data = os.path.join(base_directory, \"Meta_data\") \n",
    "directory_output = os.path.join(base_directory, \"Output\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-behavior",
   "metadata": {},
   "source": [
    "#### 2.4 Specify document / subfolder logic\n",
    "Creates for each subfolder a specific dictionary to specify which files should be gathered in which folder <br>\n",
    "Files are being recognised based on specific combinations of strings in their file name and then associated with the directory of the subfolder <br>\n",
    "\n",
    "The unique word before the variable name (here: RFP+), should be adjusted before the variable name, if needed <br>\n",
    "\n",
    "A second unique key (here: AllS or SelS) is added to choose the correct file, <br>\n",
    "if there is no differentation made for morphological data and data on number and distribution, then this second key can be the same for all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "backed-myanmar",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_destination = {\"Meta_data\": [[\"_Overview.\"], directory_meta_data, \"\"], \n",
    "                    \"Dendrite\": [[\"RFP+_Dendrite_Spine_Density.\", \"RFP+_Dendrite_Length.\"], directory_dendrite, \"AllS\"], \n",
    "                    \"Morph\": [[\"RFP+_Spine_Length.\", \"RFP+_Spine_Part_Max_Diameter_Head.\", \"RFP+_Spine_Part_Max_Diameter_Neck.\",\n",
    "                              \"RFP+_Spine_Part_Volume_Head.\", \"RFP+_Spine_Part_Volume_Neck.\", \"RFP+_Spine_Volume.\", \n",
    "                              \"RFP+_Spine_Part_Mean_Diameter_Head.\", \"RFP+_Spine_Part_Mean_Diameter_Neck.\"], directory_morph, \n",
    "                              \"SelS\"], \n",
    "                    \"Distr\": [[\"RFP+_Spine_Attachment_Pt_Distance.\", \"RFP+_Spine_Attachment_Pt_Diameter.\"], directory_distr, \n",
    "                              \"AllS\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-fight",
   "metadata": {},
   "source": [
    "#### Function: Find files and copy to the right subfolder\n",
    "The aim of this function is to copy the correct files into their subfolders based on the unique combination of parts of the file name <br>\n",
    "\n",
    "*Arguments*: <br>\n",
    "**input_folder**: folder that will be inspected to find required files <br>\n",
    "**destination_folder** : folder where files are copied to <br>\n",
    "**name_in_filename**: string that must be in the document name to be copied <br>\n",
    "**name2_in_filename**: second string that must be in the document name to be copied <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "conditional-therapist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_data_to_subfolder(input_folder, destination_folder, name_in_filename, name2_in_filename):\n",
    "    for root, dirs, files in os.walk(input_folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv') and name_in_filename in file and name2_in_filename in file: \n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    shutil.copy(file_path, destination_folder)\n",
    "                except shutil.SameFileError:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-acceptance",
   "metadata": {},
   "source": [
    "#### Function: Fill subfolders\n",
    "Function uses the file_destination dictionary from 2.4 that provides the subfolders as keys and the two search strings and subfolderpaths as values. <br> It loops through the folders, searches the required documents and copies them to the folders using the copy_data_to_subfolders function.<br>\n",
    "*Arguments*: <br><br>\n",
    "**input_folder**: folder that will be inspected to find required files <br>\n",
    "**destination_folder** : folder where files are copied to <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "available-pottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_subfolders(destination_dict, input_folder):\n",
    "    for key in destination_dict.keys():\n",
    "        for name_in_filename in destination_dict[key][0]:\n",
    "            destination_folder = destination_dict[key][1]\n",
    "            name2_in_filename = destination_dict[key][2]\n",
    "            copy_data_to_subfolder(input_folder, destination_folder, name_in_filename, name2_in_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-shore",
   "metadata": {},
   "source": [
    "#### 2.5 Fill subfolders with morph, number and dendrite data\n",
    "In this step, the above specified functions are being used to copy the desired files into their respective \n",
    "subfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "rising-bones",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_subfolders(file_destination, script_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-arbor",
   "metadata": {},
   "source": [
    "## 3. Merge and save data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-hygiene",
   "metadata": {},
   "source": [
    "#### Function: Remove second last value in unique key\n",
    "This function accepts a string and converts it into a new string. <br>\n",
    "For example 'aaa_bbb_ccc' --> 'aaa_ccc' <br>\n",
    "This is needed later when merging df_distr and df_dendrite based on key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "nonprofit-fortune",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_second_last(value):\n",
    "    # Split the string by underscores\n",
    "    parts = value.split('_')\n",
    "    # Remove the second last element\n",
    "    if len(parts) > 1:  # Check to ensure there's a second last element\n",
    "        parts.pop(-2)\n",
    "    # Join the remaining parts back into a string\n",
    "    return '_'.join(parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chief-buffer",
   "metadata": {},
   "source": [
    "#### Function: Create a list with all name info from a file and the filename\n",
    "This function separates the meta data contained in the file name based on \"_\" <br>\n",
    "also other seperators can be used <br> \n",
    "information in the file name will be used in a later step to create a unique identification key for each dendrite/spine \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "familiar-bryan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document_info(directory):\n",
    "    data_files = []\n",
    "    # Iterate over files in directory\n",
    "    for name in os.listdir(directory):\n",
    "        # Open file\n",
    "        split_name = name.split('_')\n",
    "        split_name.append(name)\n",
    "        data_files.append(split_name)\n",
    "    return data_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-series",
   "metadata": {},
   "source": [
    "#### Function: Open a csv file and return it as a Pandas dataframe\n",
    "This function opens a csv file, skips the first two rows if wanted so (required for Imaris files) and creates a unique key for each dendrite/spine <br>\n",
    "Values on dendrite level (e.g. spine density and dendrite length) only contain a Filament ID <br>\n",
    "Values on spine level (e.g. spine head volume and spine attachment Pt distance) contain a Filament ID and a spine (ID) <br>\n",
    "\n",
    "*Arguments*: <br>\n",
    "**file_name_data**: input from create_document_info function <br>\n",
    "**directory** : path to folder of the file <br>\n",
    "**remove_id_key**: option to remove the id from the key in the resulting dataframe <br>\n",
    "**key_creation**: option to remove unique_ID creation in the resulting dataframe <br>\n",
    "**skip2rows**: option to skip first 2 rows of the input csv file <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "obvious-exploration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_csv(file_name_data, directory, remove_id_key=False, key_creation=True, skip2rows=True): # remove_id_key only works if key_creation = True\n",
    "    document_path = os.path.join(directory, file_name_data[-1])\n",
    "    if skip2rows:\n",
    "        df = pd.read_csv(document_path, skiprows=[1, 2])\n",
    "    else:\n",
    "        df = pd.read_csv(document_path)\n",
    "    if key_creation:\n",
    "        df['unique_ID'] = f\"{file_name_data[1]}_{file_name_data[2]}_{file_name_data[3]}_{file_name_data[5]}_{file_name_data[8]}\"\n",
    "        if remove_id_key:\n",
    "            df['key'] = df['unique_ID'] + '_' + df['FilamentID'].astype(str)\n",
    "        else:\n",
    "            df['key'] = df['unique_ID'] + '_' + df['ID'].astype(str) + '_' + df['FilamentID'].astype(str)\n",
    "        df = df.iloc[:, [0, -1]]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-honduras",
   "metadata": {},
   "source": [
    "#### Function: Add the condition of the animal to the target dataframe based on the animal/condition table\n",
    "This function returns the dataframe with the condition added in the last column matched by the subject number (here: animal) based on the provided meta data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fixed-sequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_animal_condition_to_df(target_df, directory_meta_data):\n",
    "    meta_data_document_data = create_document_info(directory_meta_data)[0]\n",
    "    df_condition = open_csv(meta_data_document_data, directory_meta_data, key_creation=False, skip2rows=False)\n",
    "    for index, row in target_df.iterrows():\n",
    "        animal = row['key'].split('_')[0]\n",
    "        condition_animal_value = df_condition.loc[df_condition['animal'] == animal, 'condition'].values[0]\n",
    "        try:\n",
    "            target_df.at[index, 'Condition'] = condition_animal_value\n",
    "        except:\n",
    "            target_df.at[index, 'Condition'] = None\n",
    "    \n",
    "    return target_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-latino",
   "metadata": {},
   "source": [
    "#### Function: Add columns to the dataframe that can help slicing the data\n",
    "Columns that are added are *animal*, *image*, *dendrite* and *reactivation_status*.\n",
    "\n",
    "Rename variables according to order in file name as specified in open_csv function (see function above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "suitable-chile",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_slicing_columns_to_dataframe(df):\n",
    "    df[['animal', 'slice', 'side', 'layer', 'reactivation_status', 'id', 'dendrite_id']] = df['key'].str.split('_', expand=True)\n",
    "    df['image'] = df['animal'] + '_' + df['slice'] + '_' + df['side'] + '_' + df['layer']\n",
    "    df['dendrite'] = df['image'] + '_' + df['dendrite_id']\n",
    "    df = df.drop(columns=['slice', 'side', 'layer', 'id', 'dendrite_id'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-praise",
   "metadata": {},
   "source": [
    "#### Function: Merge data from a folder and provide output in a dataframe\n",
    "For example: This function can loop through all files in the Morph folder and merges it in one dataframe. \n",
    "Then the function uses the add_anmial_condition_to_df function to also add the condition of the animal. Another option is to add auxiliary columns for enhanced data slicing later. Lastly the function converts all letters to lower case. \n",
    "\n",
    "*Arguments*: <br>\n",
    "**directory** : path to folder of the file <br>\n",
    "**remove_id_key**: option to remove the id from the key in the resulting dataframe <br>\n",
    "**add_slicing_columns**: option to add auxiliary columns for example for slicing, calculations or data manipulations  <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "heated-forestry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(directory, remove_id_key=False, add_slicing_columns=False):\n",
    "    first_iteration = True\n",
    "    datafiles = create_document_info(directory)\n",
    "    for datafile in datafiles:\n",
    "        if first_iteration:\n",
    "            df = open_csv(datafile, directory, remove_id_key)\n",
    "            first_iteration = False\n",
    "        else:\n",
    "            df_add = open_csv(datafile, directory, remove_id_key)\n",
    "            df = pd.concat([df, df_add])\n",
    "    df = df.groupby('key').agg(lambda x: x.dropna().tolist()[0] if x.dropna().tolist() else None).reset_index()\n",
    "    df = add_animal_condition_to_df(df, directory_meta_data)\n",
    "    if add_slicing_columns:\n",
    "        df = add_slicing_columns_to_dataframe(df)\n",
    "    df = df.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-volume",
   "metadata": {},
   "source": [
    "#### Function: Consolidate data and save output\n",
    "This function calls the create_dataframe function and saves the consolidated data as .xlsx file in the output directory.\n",
    "\n",
    "*Arguments*: <br>\n",
    "**input_directory**: input from create_document_info function <br>\n",
    "**output_directory**: path to output folder where the files are saved <br>\n",
    "**name**: name of the Excel file  <br>\n",
    "**create_df**: option to block automatic df creation <br>\n",
    "**df**: option to manually input a df, if create_df == False <br>\n",
    "**remove_id_key**: option to remove the id from the key in the resulting dataframe <br>\n",
    "**add_slicing_columns**: option to add auxiliary columns for example for slicing, calculations or data manipulations  <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "adjustable-adoption",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_data(input_directory, output_directory, name, create_df=True, df=None, remove_id_key=False, add_slicing_columns=False):\n",
    "    if create_df:\n",
    "        df = create_dataframe(input_directory, remove_id_key, add_slicing_columns)\n",
    "    df.to_excel(os.path.join(output_directory, f\"{name}.xlsx\"), index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-member",
   "metadata": {},
   "source": [
    "#### 3.1 Consolidate and save morph, number and dendrite data\n",
    "Here, the final .xlsx data files are created based on the specific subfolders and then stored in the output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "saved-accident",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_morph = store_data(directory_morph, directory_output, 'morph_data', add_slicing_columns=True)\n",
    "df_distr = store_data(directory_distr, directory_output, 'distr_data', add_slicing_columns=True)\n",
    "df_dendrite = store_data(directory_dendrite, directory_output, 'dendrite_data', remove_id_key=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-capital",
   "metadata": {},
   "source": [
    "#### 3.2 Create seperate file that has the number data enriched with the dendrite data\n",
    "In a last step, the data frame on the distribution of spines is merged with the data frame on dendrite level, so that finally only two data files are needed for statistical analysis. <br>\n",
    "The first data file can be used for analysis on the number and distribution of spines. <br>\n",
    "The second data file can be used for analysis on the morphology of spines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "statistical-complaint",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_distr['key_dendrite'] = df_distr['key'].apply(remove_second_last)\n",
    "df_distr = df_distr.drop(columns=['Condition'])\n",
    "df_dendrite.rename(columns={'key': 'key_dendrite'}, inplace=True)\n",
    "merged_df = pd.merge(df_distr, df_dendrite, on='key_dendrite', how='left')\n",
    "df_distr_dendrite = merged_df.drop(columns=['key_dendrite'])\n",
    "store_data(directory_distr_dendrite, directory_output, 'distr_dendrite_data', create_df=False, df=df_distr_dendrite)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9452bdc2",
   "metadata": {},
   "source": [
    "The final data files can be found in the output folder and imported into R. <br> \n",
    "See R script in this repository to continue with analysis on spine parameters. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
